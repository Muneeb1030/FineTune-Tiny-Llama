{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM FINE TUNNNING ON PERSONAL DATA SET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part I: Webscrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for WebScrapper\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Web Scrapper to extract the links to Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Target URL\n",
    "driver.get(\"https://scholar.google.com/citations?user=unGWVYMAAAAJ&hl=en\")\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[@class='gsc_a_t']/a\")))\n",
    "\n",
    "# Define a function to extract PDF links from a given page\n",
    "def extract_pdf_links(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    all_links = soup.find_all('a')\n",
    "    pdf_links = [pdf_link['href'] for pdf_link in all_links if pdf_link.get('href') and ('.pdf' in pdf_link['href'] or 'pdf' in pdf_link['href'].lower())]\n",
    "    return pdf_links\n",
    "\n",
    "# Scroll to the end of page to load all results\n",
    "show_more_button = driver.find_element(By.XPATH, \"//*[@id='gsc_bpf_more']//*[@class='gs_lbl']\")\n",
    "show_more_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Find all search result links\n",
    "search_result_links = driver.find_elements(By.XPATH, \"//*[@class='gsc_a_t']/a\")\n",
    "\n",
    "# List to store all PDF links\n",
    "all_pdf_links = []\n",
    "\n",
    "# Loop through each link\n",
    "for link in search_result_links:\n",
    "    # Get the href attribute of the link\n",
    "    href = link.get_attribute(\"href\")\n",
    "    # Store the link URL before clicking\n",
    "    link_url = href\n",
    "    # Open the link in a new tab using JavaScript to avoid navigation\n",
    "    driver.execute_script(\"window.open('\" + link_url + \"');\")\n",
    "    # Switch to the newly opened tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    # Wait for the page to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//*\")))\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    # Extract PDF links from the page source\n",
    "    pdf_links = extract_pdf_links(page_source)\n",
    "    # Print PDF links\n",
    "    if pdf_links:\n",
    "        print(\"PDF links on\", href, \":\", pdf_links)\n",
    "        all_pdf_links.extend(pdf_links)\n",
    "        \n",
    "        # Remove dulicate links\n",
    "        all_pdf_links = list(set(all_pdf_links))\n",
    "        \n",
    "    else:\n",
    "        print(\"No PDF links found on\", href)\n",
    "    # Close the current tab\n",
    "    driver.close()\n",
    "    # Switch back to the main tab\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Downloading and Renaming the Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write links to file\n",
    "with open('pdf_links.txt', 'w') as f:\n",
    "    for pdf_link in all_pdf_links:\n",
    "        f.write(pdf_link + '\\n')\n",
    "        \n",
    "# Create folder for files\n",
    "folder_path = \"pdf_folder\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# read links from file\n",
    "with open('pdf_links.txt', 'r') as f:\n",
    "    all_pdf_links = f.read().splitlines()\n",
    "    \n",
    "# Initialize counter\n",
    "file_counter = 1\n",
    "\n",
    "# Download PDF files\n",
    "for pdf_link in all_pdf_links:\n",
    "    if pdf_link.startswith('http'):\n",
    "        pdf_url = pdf_link\n",
    "    else:\n",
    "        pdf_url = urljoin(href, pdf_link)\n",
    "    \n",
    "    # Download PDF file\n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    # Ensure the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Determine file name\n",
    "        file_name = os.path.join(folder_path, f\"{file_counter}.pdf\")\n",
    "        \n",
    "        # Save the PDF file\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(\"Downloaded:\", file_name)\n",
    "        \n",
    "        # Increment counter for the next file\n",
    "        file_counter += 1\n",
    "    else:\n",
    "        print(f\"Failed to download {pdf_url}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **----------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part II: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for PDF to Text\n",
    "import os\n",
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing unwanted lines and references.\"\"\"\n",
    "    text = re.sub(r'\\[(.*?)\\]', '', text)  # Remove reference notations\n",
    "    lines = text.split('\\n')\n",
    "    return [line for line in lines if not line.lower().startswith(('fig', 'fig.', 'figure', 'table'))]\n",
    "\n",
    "def join_lines(lines):\n",
    "    \"\"\"Join lines properly to maintain paragraph integrity.\"\"\"\n",
    "    filtered_lines = []\n",
    "    prev_line = \"\"\n",
    "    for line in lines:\n",
    "        if prev_line and not prev_line.endswith('.') and not line.startswith((' ', '\\t')):\n",
    "            filtered_lines[-1] += ' ' + line\n",
    "        else:\n",
    "            filtered_lines.append(line)\n",
    "        prev_line = line\n",
    "    return '\\n\\n'.join(filtered_lines)  # Double newline to signify paragraph end\n",
    "\n",
    "def extract_text_from_pdf(pdf_file, output_txt):\n",
    "    text_between_sections = \"\"\n",
    "    abstract_started = False\n",
    "    references_started = False\n",
    "    try:\n",
    "        with fitz.open(pdf_file) as doc:\n",
    "            for page in doc:\n",
    "                for table in page.find_tables():\n",
    "                    page.add_redact_annot(table.bbox)\n",
    "                page.apply_redactions()\n",
    "                text = page.get_text(\"text\")\n",
    "\n",
    "                if \"abstract\" in text.lower() and not abstract_started:\n",
    "                    abstract_started = True\n",
    "                    text = text[(text.lower()).index(\"abstract\"):]\n",
    "\n",
    "                if \"REFERENCES\" in text and abstract_started:\n",
    "                    references_started = True\n",
    "                    text = text[:text.index(\"REFERENCES\")]\n",
    "\n",
    "                if abstract_started:\n",
    "                    cleaned_lines = clean_text(text)\n",
    "                    text_between_sections += join_lines(cleaned_lines)\n",
    "\n",
    "                if abstract_started and references_started:\n",
    "                    break\n",
    "\n",
    "        with open(output_txt, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text_between_sections)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {pdf_file}: {str(e)}\")\n",
    "\n",
    "def process_pdf_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            output_txt = os.path.splitext(pdf_path)[0] + \".txt\" \n",
    "            extract_text_from_pdf(pdf_path, output_txt)\n",
    "            logging.info(f\"Processed {filename}\")\n",
    "\n",
    "process_pdf_folder(\"Pdf_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part III: Preparing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Data to get one file\n",
    "import os\n",
    "\n",
    "def load_and_concatenate_txt(directory):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                all_texts.append(text.strip())\n",
    "    return '\\n\\n'.join(all_texts)  # Join all articles with double newlines\n",
    "\n",
    "# Specify the directory where your text files are stored\n",
    "directory = 'pdf_folder/TextFiles'\n",
    "full_corpus = load_and_concatenate_txt(directory)\n",
    "\n",
    "# Save the full corpus to a text file\n",
    "output_file = 'output.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\FineTune\\venv\\Lib\\site-packages\n",
      "                                               input  \\\n",
      "0  Write a Paragraph road safety, travel comfort,...   \n",
      "1  Write a Paragraph adversarial ML attacks, ML, ...   \n",
      "2                      Write a Paragraph Index Terms   \n",
      "3  Write a Paragraph conventional connected vehic...   \n",
      "4  Write a Paragraph connected vehicles, vehicula...   \n",
      "\n",
      "                                              output  \n",
      "0  Abstract—Connected and autonomous vehicles (CA...  \n",
      "1  Such a transformation—which will be fuelled by...  \n",
      "2  Index Terms—Connected and autonomous vehicles,...  \n",
      "3  I. INTRODUCTION In recent years, connected and...  \n",
      "4  The phenomenon of connected vehicles is realiz...  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add pytextrank to the pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "def generate_prompts(texts):\n",
    "    data = {\n",
    "        'input': [],\n",
    "        'output': []\n",
    "    }\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        # Extract key phrases; adjust the limit as needed\n",
    "        key_phrases = ', '.join([phrase.text for phrase in doc._.phrases[:3]])\n",
    "        prompt = f\"Write a Paragraph {key_phrases}\"\n",
    "        data['input'].append(prompt)\n",
    "        data['output'].append(text)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "with open('output.txt', 'r', encoding='utf-8') as file:\n",
    "    paragraphs = [para.strip() for para in file.readlines() if para.strip()]\n",
    "    \n",
    "df = generate_prompts(paragraphs)\n",
    "print(df.head())\n",
    "# Optionally, save to a file\n",
    "df.to_csv('training_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file, json_file):\n",
    "    data = []\n",
    "    with open(csv_file, 'r', encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            input_text = row['input']\n",
    "            output_text = row['output']\n",
    "            data.append({\n",
    "                'instruction': input_text,\n",
    "                'query': '',\n",
    "                'prompt': output_text\n",
    "            })\n",
    "    \n",
    "    with open(json_file, 'w') as jsonfile:\n",
    "        json.dump(data, jsonfile, indent=4)\n",
    "\n",
    "csv_file = \"training_data.csv\"\n",
    "json_file = \"output.json\"\n",
    "csv_to_json(csv_file, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **--------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Text Analysis\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Define a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Define a function to perform stemming\n",
    "def perform_stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [ps.stem(word) for word in word_tokens]\n",
    "    return ' '.join(stemmed_text)\n",
    "\n",
    "# Define a function to plot word frequency\n",
    "def plot_word_frequency(text, num_words=10):\n",
    "    fdist = FreqDist(text.split())\n",
    "    fdist = dict(sorted(fdist.items(), key=lambda x: x[1], reverse=True))\n",
    "    words, frequencies = list(fdist.keys())[:num_words], list(fdist.values())[:num_words]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Word Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    keywords = [word for word, tag in tagged_words if tag in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "    return keywords\n",
    "\n",
    "# Process text files in a folder\n",
    "def process_text_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                processed_text = preprocess_text(text)\n",
    "                processed_text = remove_stopwords(processed_text)\n",
    "                processed_text = perform_stemming(processed_text)\n",
    "                keywords = extract_keywords(processed_text)\n",
    "                print(f\"Keywords in {filename}: {keywords}\")\n",
    "                plot_word_frequency(processed_text)\n",
    "                \n",
    "                \n",
    "process_text_folder(\"pdf_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "\n",
    "def extract_text_between_sections(pdf_file):\n",
    "    text_between_sections = \"\"\n",
    "    abstract_started = False\n",
    "    references_started = False\n",
    "\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            \n",
    "\n",
    "            # Remove table texts from the page text\n",
    "            for table in page.find_tables():\n",
    "                page.add_redact_annot(table.bbox)  # wrap table in a redaction annotation\n",
    "\n",
    "            page.apply_redactions()\n",
    "            \n",
    "            text = page.get_text(\"text\")  # Extract text from the page\n",
    "            # text = text.lower()\n",
    "\n",
    "            if \"abstract\" in text.lower() and not abstract_started:\n",
    "                abstract_started = True\n",
    "                text = text[(text.lower()).index(\"abstract\"):]\n",
    "\n",
    "            if \"REFERENCES\" in text and abstract_started:\n",
    "                references_started = True\n",
    "                text = text[:text.index(\"REFERENCES\")]\n",
    "\n",
    "            if abstract_started:\n",
    "                # Remove reference mentions (including square brackets and numbers)\n",
    "                text = re.sub(r'\\[(.*?)\\]', '', text)\n",
    "\n",
    "                # Remove lines starting with \"fig\", \"fig.\", \"figure\", or \"table\"\n",
    "                lines = text.split('\\n')\n",
    "                filtered_lines = [line for line in lines if not line.lower().startswith(('fig', 'fig.', 'figure', 'table'))]\n",
    "                text = '\\n'.join(filtered_lines)\n",
    "\n",
    "                text_between_sections += text\n",
    "\n",
    "\n",
    "            if abstract_started and references_started:\n",
    "                break\n",
    "    return text_between_sections\n",
    "\n",
    "pdf_file = 'pdf_folder/1.pdf'\n",
    "text_between_sections = extract_text_between_sections(pdf_file)\n",
    "print(text_between_sections)\n",
    "\n",
    "# write to file\n",
    "output_txt = 'output.txt'\n",
    "with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "    f.write(text_between_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def extract_sections(pdf_file):\n",
    "    text_between_sections = \"\"\n",
    "    abstract_started = False\n",
    "    references_started = False\n",
    "\n",
    "    with fitz.open(pdf_file) as doc:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            # Remove table texts from the page text\n",
    "            for table in page.find_tables():\n",
    "                page.add_redact_annot(table.bbox)  # wrap table in a redaction annotation\n",
    "\n",
    "            page.apply_redactions()\n",
    "            \n",
    "            text_blocks = page.get_text(\"blocks\")\n",
    "\n",
    "            for block in text_blocks:\n",
    "                if block[4].strip().isdigit():\n",
    "                    continue\n",
    "                elif (block[4].lower()).startswith(\"abstract\"):\n",
    "                    abstract_started = True\n",
    "                elif (block[4].lower()).startswith(\"references\"):\n",
    "                    break\n",
    "                \n",
    "                if abstract_started:\n",
    "                    # remove \\n with the block[4] and only keep the last \\n\n",
    "                    # text = block[4].replace('\\n', ' ')\n",
    "                    # text = text.strip()\n",
    "                    \n",
    "                    # text_between_sections += text + '\\n'\n",
    "                    print(block[4])\n",
    "\n",
    "pdf_file = \"pdf_folder/1.pdf\"\n",
    "sections = extract_sections(pdf_file)\n",
    "\n",
    "# for section in sections:\n",
    "#     print(f\"Level {section['level']}: {section['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
