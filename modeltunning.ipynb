{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLM FINE TUNNNING ON PERSONAL DATA SET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part I: Webscrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for WebScrapper\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Web Scrapper to extract the links to Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Target URL\n",
    "driver.get(\"https://scholar.google.com/citations?user=unGWVYMAAAAJ&hl=en\")\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//*[@class='gsc_a_t']/a\")))\n",
    "\n",
    "# Define a function to extract PDF links from a given page\n",
    "def extract_pdf_links(page_source):\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    all_links = soup.find_all('a')\n",
    "    pdf_links = [pdf_link['href'] for pdf_link in all_links if pdf_link.get('href') and ('.pdf' in pdf_link['href'] or 'pdf' in pdf_link['href'].lower())]\n",
    "    return pdf_links\n",
    "\n",
    "# Scroll to the end of page to load all results\n",
    "show_more_button = driver.find_element(By.XPATH, \"//*[@id='gsc_bpf_more']//*[@class='gs_lbl']\")\n",
    "show_more_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Find all search result links\n",
    "search_result_links = driver.find_elements(By.XPATH, \"//*[@class='gsc_a_t']/a\")\n",
    "\n",
    "# List to store all PDF links\n",
    "all_pdf_links = []\n",
    "\n",
    "# Loop through each link\n",
    "for link in search_result_links:\n",
    "    # Get the href attribute of the link\n",
    "    href = link.get_attribute(\"href\")\n",
    "    # Store the link URL before clicking\n",
    "    link_url = href\n",
    "    # Open the link in a new tab using JavaScript to avoid navigation\n",
    "    driver.execute_script(\"window.open('\" + link_url + \"');\")\n",
    "    # Switch to the newly opened tab\n",
    "    driver.switch_to.window(driver.window_handles[-1])\n",
    "    # Wait for the page to load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//*\")))\n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    # Extract PDF links from the page source\n",
    "    pdf_links = extract_pdf_links(page_source)\n",
    "    # Print PDF links\n",
    "    if pdf_links:\n",
    "        print(\"PDF links on\", href, \":\", pdf_links)\n",
    "        all_pdf_links.extend(pdf_links)\n",
    "        \n",
    "        # Remove dulicate links\n",
    "        all_pdf_links = list(set(all_pdf_links))\n",
    "        \n",
    "    else:\n",
    "        print(\"No PDF links found on\", href)\n",
    "    # Close the current tab\n",
    "    driver.close()\n",
    "    # Switch back to the main tab\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Downloading and Renaming the Articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write links to file\n",
    "with open('pdf_links.txt', 'w') as f:\n",
    "    for pdf_link in all_pdf_links:\n",
    "        f.write(pdf_link + '\\n')\n",
    "        \n",
    "# Create folder for files\n",
    "folder_path = \"pdf_folder\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# read links from file\n",
    "with open('pdf_links.txt', 'r') as f:\n",
    "    all_pdf_links = f.read().splitlines()\n",
    "    \n",
    "# Initialize counter\n",
    "file_counter = 1\n",
    "\n",
    "# Download PDF files\n",
    "for pdf_link in all_pdf_links:\n",
    "    if pdf_link.startswith('http'):\n",
    "        pdf_url = pdf_link\n",
    "    else:\n",
    "        pdf_url = urljoin(href, pdf_link)\n",
    "    \n",
    "    # Download PDF file\n",
    "    response = requests.get(pdf_url)\n",
    "    \n",
    "    # Ensure the response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Determine file name\n",
    "        file_name = os.path.join(folder_path, f\"{file_counter}.pdf\")\n",
    "        \n",
    "        # Save the PDF file\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(\"Downloaded:\", file_name)\n",
    "        \n",
    "        # Increment counter for the next file\n",
    "        file_counter += 1\n",
    "    else:\n",
    "        print(f\"Failed to download {pdf_url}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **----------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part II: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for PDF to Text\n",
    "import os\n",
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing unwanted lines and references.\"\"\"\n",
    "    text = re.sub(r'\\[(.*?)\\]', '', text)  # Remove reference notations\n",
    "    lines = text.split('\\n')\n",
    "    return [line for line in lines if not line.lower().startswith(('fig', 'fig.', 'figure', 'table'))]\n",
    "\n",
    "def join_lines(lines):\n",
    "    \"\"\"Join lines properly to maintain paragraph integrity.\"\"\"\n",
    "    filtered_lines = []\n",
    "    prev_line = \"\"\n",
    "    for line in lines:\n",
    "        if prev_line and not prev_line.endswith('.') and not line.startswith((' ', '\\t')):\n",
    "            filtered_lines[-1] += ' ' + line\n",
    "        else:\n",
    "            filtered_lines.append(line)\n",
    "        prev_line = line\n",
    "    return '\\n\\n'.join(filtered_lines)  # Double newline to signify paragraph end\n",
    "\n",
    "def extract_text_from_pdf(pdf_file, output_txt):\n",
    "    text_between_sections = \"\"\n",
    "    abstract_started = False\n",
    "    references_started = False\n",
    "    try:\n",
    "        with fitz.open(pdf_file) as doc:\n",
    "            for page in doc:\n",
    "                for table in page.find_tables():\n",
    "                    page.add_redact_annot(table.bbox)\n",
    "                page.apply_redactions()\n",
    "                text = page.get_text(\"text\")\n",
    "\n",
    "                if \"abstract\" in text.lower() and not abstract_started:\n",
    "                    abstract_started = True\n",
    "                    text = text[(text.lower()).index(\"abstract\"):]\n",
    "\n",
    "                if \"REFERENCES\" in text and abstract_started:\n",
    "                    references_started = True\n",
    "                    text = text[:text.index(\"REFERENCES\")]\n",
    "\n",
    "                if abstract_started:\n",
    "                    cleaned_lines = clean_text(text)\n",
    "                    text_between_sections += join_lines(cleaned_lines)\n",
    "\n",
    "                if abstract_started and references_started:\n",
    "                    break\n",
    "\n",
    "        with open(output_txt, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(text_between_sections)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {pdf_file}: {str(e)}\")\n",
    "\n",
    "def process_pdf_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            output_txt = os.path.splitext(pdf_path)[0] + \".txt\" \n",
    "            extract_text_from_pdf(pdf_path, output_txt)\n",
    "            logging.info(f\"Processed {filename}\")\n",
    "\n",
    "process_pdf_folder(\"Pdf_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part III: Preparing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Data to get one file\n",
    "import os\n",
    "\n",
    "def load_and_concatenate_txt(directory):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                all_texts.append(text.strip())\n",
    "    return '\\n\\n'.join(all_texts)  # Join all articles with double newlines\n",
    "\n",
    "# Specify the directory where your text files are stored\n",
    "directory = 'pdf_folder/TextFiles'\n",
    "full_corpus = load_and_concatenate_txt(directory)\n",
    "\n",
    "# Save the full corpus to a text file\n",
    "output_file = 'output.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from api import API_KEY\n",
    "client = OpenAI(api_key=API_KEY)  \n",
    "\n",
    "def label_data(data):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    you will write a neutral text for the provided text. \n",
    "    Text: \"\n",
    "       {data}\n",
    "       \"\n",
    "Note: Neutral text means how you will write about the same topic, discussed in paragraph. Write the text in your style which will cover all the things and I'm calling your written Text \"Neutral Text \". So your task is to write a neutral text other things are for your assistant. Write like its your own text not mention words like \"the paragraph discusses.\" you will write like you are writing for first time   \n",
    "\n",
    "Response: response will only contain neutral text no explanation nothing else. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    response = completion.choices[0].message.content  \n",
    "    return response\n",
    "\n",
    "def append_to_json_and_file(data, label, json_file_path, data_file_path):\n",
    "    try:\n",
    "        # Load existing data from the JSON file\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data_list = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, start with an empty list\n",
    "        data_list = []\n",
    "\n",
    "    # Append the new data and label to the list\n",
    "    data_list.append({ \"prompt\": label, \"ans\": data})\n",
    "\n",
    "    # Write the updated list back to the JSON file\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data_list, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Append the new data and label to the data file\n",
    "    with open(data_file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(f\"{data}\\n{label}\\n\\n\")\n",
    "\n",
    "def process_file(file_path, json_file_path, data_file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        \n",
    "        paragraphs = content.split(\"\\n\\n\")\n",
    "        start_time = time.time()\n",
    "        for i, s in enumerate(paragraphs):\n",
    "            iteration_start_time = time.time()\n",
    "            if i > 0:\n",
    "                print(i)\n",
    "                label = label_data(s)\n",
    "                append_to_json_and_file(s, label, json_file_path, data_file_path)\n",
    "                # Calculate the delay needed to achieve approximately 3 iterations per minute      \n",
    "                delay_seconds = 60 / 3 - (time.time() - start_time) % (60 / 3)\n",
    "                if delay_seconds > 0:\n",
    "                    time.sleep(delay_seconds)\n",
    "            iteration_end_time = time.time()    \n",
    "            processing_time = iteration_end_time - iteration_start_time\n",
    "            print(f\"Processing time for iteration {i}: {processing_time:.2f} seconds\")\n",
    "            \n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def process_files(file_paths, json_file_path, data_file_path):\n",
    "    for file_path in file_paths:\n",
    "        process_file(file_path, json_file_path, data_file_path)\n",
    "\n",
    "# Example usage\n",
    "file_paths =   [\"cleaned_14-19.txt\"]\n",
    "json_file_path = \"14-19.json\"\n",
    "data_file_path = \"14-19.txt\"\n",
    "\n",
    "process_files(file_paths, json_file_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\FineTune\\venv\\Lib\\site-packages\n",
      "                                               input  \\\n",
      "0  Write a Paragraph road safety, travel comfort,...   \n",
      "1  Write a Paragraph adversarial ML attacks, ML, ...   \n",
      "2                      Write a Paragraph Index Terms   \n",
      "3  Write a Paragraph conventional connected vehic...   \n",
      "4  Write a Paragraph connected vehicles, vehicula...   \n",
      "\n",
      "                                              output  \n",
      "0  Abstract—Connected and autonomous vehicles (CA...  \n",
      "1  Such a transformation—which will be fuelled by...  \n",
      "2  Index Terms—Connected and autonomous vehicles,...  \n",
      "3  I. INTRODUCTION In recent years, connected and...  \n",
      "4  The phenomenon of connected vehicles is realiz...  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add pytextrank to the pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "def generate_prompts(texts):\n",
    "    data = {\n",
    "        'input': [],\n",
    "        'output': []\n",
    "    }\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        # Extract key phrases; adjust the limit as needed\n",
    "        key_phrases = ', '.join([phrase.text for phrase in doc._.phrases[:3]])\n",
    "        prompt = f\"Write a Paragraph {key_phrases}\"\n",
    "        data['input'].append(prompt)\n",
    "        data['output'].append(text)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "with open('output.txt', 'r', encoding='utf-8') as file:\n",
    "    paragraphs = [para.strip() for para in file.readlines() if para.strip()]\n",
    "    \n",
    "df = generate_prompts(paragraphs)\n",
    "print(df.head())\n",
    "# Optionally, save to a file\n",
    "df.to_csv('training_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_file, json_file):\n",
    "    data = []\n",
    "    with open(csv_file, 'r', encoding=\"utf-8\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            input_text = row['input']\n",
    "            output_text = row['output']\n",
    "            data.append({\n",
    "                'instruction': input_text,\n",
    "                'query': '',\n",
    "                'prompt': output_text\n",
    "            })\n",
    "    \n",
    "    with open(json_file, 'w') as jsonfile:\n",
    "        json.dump(data, jsonfile, indent=4)\n",
    "\n",
    "csv_file = \"training_data.csv\"\n",
    "json_file = \"output.json\"\n",
    "csv_to_json(csv_file, json_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
